{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Notes: \n",
    "* Answer each question in a separate Jupynet Notebook Cell\n",
    "* Pleas keep the code in your cells short. \n",
    "  * In notebook programming cells are typicaly short to facilitate reading. \n",
    "  * If well thought out, most answers in this assignment won're require more than 3 or 4 lines of code. \n",
    "* Do no change the list of import, i.e., do not add additional libraries. Those included are the only ones you are allowed to use.\n",
    "* Add your first and Last name below:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Nicholas \"Kiko\" Whiteley"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "source": [
    "import pandas as pd\n",
    "from itertools import product\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "import random"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this assignment you will be working with Corona Virus (SARS-CoV2) data that was obtained from the [National Center for Biotechnology Information](https://www.ncbi.nlm.nih.gov/). You will need two files. The first (`data/coronavirus_info.csv`) is small and is provided in the GitHub Repo. The second  (`data_report.jsonl`) is larger so you will need to download a compressed version, which you will need to uncompress prior to using. You can downlod the second file here:\n",
    "\n",
    "https://www.dropbox.com/s/qdn67rshygz06ff/data_report.jsonl.gz?dl=0\n",
    "\n",
    "We start by loading `data/coronavirus_info.csv` (Code provided below)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "source": [
    "# CODE PROVIDED -- DO NOT REMOVE\n",
    "table = pd.read_csv(\"data/coronavirus_info.csv\", low_memory=False)\n",
    "table = table.drop([\"US State\", \"Host Name\", \"Host Taxonomy ID\", \"Sequence Type\", \"Species Taxonomy Id\", \"Nuc Completeness\", \"BioProject\", \"BioSample\"], axis=1)\n",
    "\n",
    "missing = table[\"Geo Location\"].isnull()\n",
    "table.loc[missing, \"Geo Location\"] = \"\"\n",
    "\n",
    "table.head(10)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nucleotide Accession</th>\n",
       "      <th>Species Name</th>\n",
       "      <th>Virus Genus</th>\n",
       "      <th>Virus Family</th>\n",
       "      <th>Isolate Name</th>\n",
       "      <th>Nucleotide Length</th>\n",
       "      <th>Geo Location</th>\n",
       "      <th>Collection Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NC_045512.2</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus 2</td>\n",
       "      <td>Betacoronavirus</td>\n",
       "      <td>Coronaviridae</td>\n",
       "      <td>Wuhan-Hu-1</td>\n",
       "      <td>29903</td>\n",
       "      <td>Asia; China</td>\n",
       "      <td>2019-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK058807.1</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus 2</td>\n",
       "      <td>Betacoronavirus</td>\n",
       "      <td>Coronaviridae</td>\n",
       "      <td>SARS-CoV-2/human/USA/MA-MASPHL-04825/2021</td>\n",
       "      <td>29801</td>\n",
       "      <td>North America; USA</td>\n",
       "      <td>2021-07-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OK058777.1</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus 2</td>\n",
       "      <td>Betacoronavirus</td>\n",
       "      <td>Coronaviridae</td>\n",
       "      <td>SARS-CoV-2/human/USA/MA-MASPHL-04790/2021</td>\n",
       "      <td>29771</td>\n",
       "      <td>North America; USA</td>\n",
       "      <td>2021-08-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OK058695.1</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus 2</td>\n",
       "      <td>Betacoronavirus</td>\n",
       "      <td>Coronaviridae</td>\n",
       "      <td>SARS-CoV-2/human/USA/MA-MASPHL-04700/2021</td>\n",
       "      <td>29820</td>\n",
       "      <td>North America; USA</td>\n",
       "      <td>2021-08-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OK058662.1</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus 2</td>\n",
       "      <td>Betacoronavirus</td>\n",
       "      <td>Coronaviridae</td>\n",
       "      <td>SARS-CoV-2/human/USA/MA-MASPHL-04651/2021</td>\n",
       "      <td>29798</td>\n",
       "      <td>North America; USA</td>\n",
       "      <td>2021-08-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OK058592.1</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus 2</td>\n",
       "      <td>Betacoronavirus</td>\n",
       "      <td>Coronaviridae</td>\n",
       "      <td>SARS-CoV-2/human/USA/MA-MASPHL-04499/2021</td>\n",
       "      <td>29802</td>\n",
       "      <td>North America; USA</td>\n",
       "      <td>2021-07-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>OK056996.1</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus 2</td>\n",
       "      <td>Betacoronavirus</td>\n",
       "      <td>Coronaviridae</td>\n",
       "      <td>SARS-CoV-2/human/USA/FL-CDC-QDX27934346/2021</td>\n",
       "      <td>29775</td>\n",
       "      <td>North America; USA: Florida</td>\n",
       "      <td>2021-08-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>OK056909.1</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus 2</td>\n",
       "      <td>Betacoronavirus</td>\n",
       "      <td>Coronaviridae</td>\n",
       "      <td>SARS-CoV-2/human/USA/CA-CDC-QDX27909662/2021</td>\n",
       "      <td>29775</td>\n",
       "      <td>North America; USA: California</td>\n",
       "      <td>2021-08-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>OK056850.1</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus 2</td>\n",
       "      <td>Betacoronavirus</td>\n",
       "      <td>Coronaviridae</td>\n",
       "      <td>SARS-CoV-2/human/USA/FL-CDC-QDX27934406/2021</td>\n",
       "      <td>29763</td>\n",
       "      <td>North America; USA: Florida</td>\n",
       "      <td>2021-08-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>OK056784.1</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus 2</td>\n",
       "      <td>Betacoronavirus</td>\n",
       "      <td>Coronaviridae</td>\n",
       "      <td>SARS-CoV-2/human/USA/NY-CDC-QDX28007789/2021</td>\n",
       "      <td>29775</td>\n",
       "      <td>North America; USA: New York</td>\n",
       "      <td>2021-08-21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Nucleotide Accession                                     Species Name  \\\n",
       "0          NC_045512.2  Severe acute respiratory syndrome coronavirus 2   \n",
       "1           OK058807.1  Severe acute respiratory syndrome coronavirus 2   \n",
       "2           OK058777.1  Severe acute respiratory syndrome coronavirus 2   \n",
       "3           OK058695.1  Severe acute respiratory syndrome coronavirus 2   \n",
       "4           OK058662.1  Severe acute respiratory syndrome coronavirus 2   \n",
       "5           OK058592.1  Severe acute respiratory syndrome coronavirus 2   \n",
       "6           OK056996.1  Severe acute respiratory syndrome coronavirus 2   \n",
       "7           OK056909.1  Severe acute respiratory syndrome coronavirus 2   \n",
       "8           OK056850.1  Severe acute respiratory syndrome coronavirus 2   \n",
       "9           OK056784.1  Severe acute respiratory syndrome coronavirus 2   \n",
       "\n",
       "       Virus Genus   Virus Family  \\\n",
       "0  Betacoronavirus  Coronaviridae   \n",
       "1  Betacoronavirus  Coronaviridae   \n",
       "2  Betacoronavirus  Coronaviridae   \n",
       "3  Betacoronavirus  Coronaviridae   \n",
       "4  Betacoronavirus  Coronaviridae   \n",
       "5  Betacoronavirus  Coronaviridae   \n",
       "6  Betacoronavirus  Coronaviridae   \n",
       "7  Betacoronavirus  Coronaviridae   \n",
       "8  Betacoronavirus  Coronaviridae   \n",
       "9  Betacoronavirus  Coronaviridae   \n",
       "\n",
       "                                   Isolate Name  Nucleotide Length  \\\n",
       "0                                    Wuhan-Hu-1              29903   \n",
       "1     SARS-CoV-2/human/USA/MA-MASPHL-04825/2021              29801   \n",
       "2     SARS-CoV-2/human/USA/MA-MASPHL-04790/2021              29771   \n",
       "3     SARS-CoV-2/human/USA/MA-MASPHL-04700/2021              29820   \n",
       "4     SARS-CoV-2/human/USA/MA-MASPHL-04651/2021              29798   \n",
       "5     SARS-CoV-2/human/USA/MA-MASPHL-04499/2021              29802   \n",
       "6  SARS-CoV-2/human/USA/FL-CDC-QDX27934346/2021              29775   \n",
       "7  SARS-CoV-2/human/USA/CA-CDC-QDX27909662/2021              29775   \n",
       "8  SARS-CoV-2/human/USA/FL-CDC-QDX27934406/2021              29763   \n",
       "9  SARS-CoV-2/human/USA/NY-CDC-QDX28007789/2021              29775   \n",
       "\n",
       "                     Geo Location Collection Date  \n",
       "0                     Asia; China         2019-12  \n",
       "1              North America; USA      2021-07-29  \n",
       "2              North America; USA      2021-08-10  \n",
       "3              North America; USA      2021-08-15  \n",
       "4              North America; USA      2021-08-09  \n",
       "5              North America; USA      2021-07-26  \n",
       "6     North America; USA: Florida      2021-08-18  \n",
       "7  North America; USA: California      2021-08-16  \n",
       "8     North America; USA: Florida      2021-08-18  \n",
       "9    North America; USA: New York      2021-08-21  "
      ]
     },
     "metadata": {},
     "execution_count": 211
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q.1\n",
    "\n",
    "* The location of each of the sequences is recorded under the `Geo Location` column.  How many entries are from Asia?\n",
    "  * Note that for some records, the `Geo Location` column is missing\n",
    "  * Display the results using the following format: \n",
    "    Asia: XXXX,\n",
    "    North America': XXXX,\n",
    "    Europe: XXXX,\n",
    "    Oceania: XXXX,\n",
    "    Africa: XXXX,\n",
    "    South America: XXXX \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "source": [
    "# Get the column with all locations\n",
    "locs = table['Geo Location']\n",
    "\n",
    "asia_count = 0\n",
    "# For each location, if the first word is 'Asia', increment the count\n",
    "for loc in locs:\n",
    "    loc_split = loc.split(';')\n",
    "    country = loc_split[0]\n",
    "    if country == 'Asia':\n",
    "        asia_count += 1\n",
    "\n",
    "print('Sequences from Asia:', asia_count)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sequences from Asia: 3903\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q.2\n",
    "Use the `coronavirus_info.csv` table to count the entries that are from Hawaii. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "source": [
    "hawaii_count = 0\n",
    "for loc in locs:\n",
    "    # Split by space\n",
    "    loc_split = loc.split()\n",
    "    # Increment count if the last word is 'Hawaii'\n",
    "    if len(loc_split) > 0:\n",
    "        last_word = loc_split[-1]\n",
    "        if last_word == 'Hawaii':\n",
    "            hawaii_count += 1\n",
    "\n",
    "print('Sequences from Hawaii:', hawaii_count)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sequences from Hawaii: 119\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q.3\n",
    "\n",
    "The file `data_report.jsonl` contains the variants of the virus in the DB. This `json` file is a list of records (one per line) for each one of the genomes in the database. Before we work with the large file, we will experiment with a file containing a single record.\n",
    "\n",
    "The file `single_record.json` contains a single sample record. Use the `JSON library to load the file `single_record.json` into a variable called `sample_vir_record`\n",
    "\n",
    "1. how many first-level keys does this record have?\n",
    "  * Do not count nested keys. Only those at the top level should be counted\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "source": [
    "import json\n",
    "\n",
    "sample_file = open('data/single_record.json')\n",
    "sample_vir_record = json.load(sample_file)\n",
    "\n",
    "# Find all first-level keys\n",
    "sample_keys = sample_vir_record.keys()\n",
    "\n",
    "print('The sample has', len(sample_keys), 'keys')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The sample has 18 keys\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q.4\n",
    "\n",
    "Each Covid various in this database is classified according to a system referred to as the Pangolin (Phylogenetic Assignment of Named Global Outbreak LINeages) classification. It is not essential to complete the assignment that you understand this system, but if you're interested in learning more, see:\n",
    "\n",
    "https://cov-lineages.org/resources/pangolin.html\n",
    "\n",
    "The Pangolin classification of this sample record is nested within the `virus` key:\n",
    "```json\n",
    "{ ...\n",
    "  \"virus\": {\n",
    "              ...\n",
    "              \"pangolinClassification\": \n",
    "              ...\n",
    "            }\n",
    "  ... \n",
    "}\n",
    "```\n",
    "Write code to extract the classification of this record. The result should be `B.1.1.214`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "source": [
    "# Access this via keys\n",
    "sample_pang_class = sample_vir_record['virus']['pangolinClassification']\n",
    "\n",
    "print('Sample pangolin classification is ', sample_pang_class)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sample pangolin classification is  B.1.1.214\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We hear in the news about `Alpha`, `Beta`, `Delta` variants of concern and recently the Mu variant as being of interest. Basedon your answer to `Q.3`, you may have been tempted to infer that this virus is of type `Beta` since the first letter is `B`. In fact, this variant of type `Alpha` and is a variant of concern. Although not relevant to this exercise, the rules for naming new variants and the list of known `SARS-CoV-2` are provided here:\n",
    "\n",
    "https://www.pango.network/how-does-the-system-work/what-are-pango-lineages/\n",
    "\n",
    "https://cov-lineages.org/lineage_list.html\n",
    "\n",
    "\n",
    "The following short video is very helpful for understanding what a variant is, how it arises, how it's named, and why some variants are more concerning than others.\n",
    "\n",
    "https://www.youtube.com/watch?v=B8UEZ9cfgz4"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q.5\n",
    "\n",
    "Write Python code that counts all the different kinds of variants in `data_report.jsonl`. \n",
    "Because the file is 7GB, you won't likely be able to load it into your laptop's RAM using Python. I encountered this error when trying to open it on my laptop\n",
    "\n",
    "![](https://www.dropbox.com/s/lieo685pafkgm5e/ram_error.png?dl=1)\n",
    "\n",
    "\n",
    "It would be easy to extract the data from the pangolinClassification field of each `json` record by reading each line (a record) at a time.\n",
    "\n",
    "The list of current variants of concern we are interested in are:\n",
    "      * Alpha (B.1.1.7)\n",
    "      * Beta (B.1.351, B.1.351.2, B.1.351.3)\n",
    "      * Delta (B.1.617.2, AY.1, AY.2, AY.3)\n",
    "      * Gamma (P.1, P.1.1, P.1.2) \n",
    "\n",
    "You should get something similar to what follows:\n",
    "```\n",
    "Alpha: X\n",
    "Beta: X\n",
    "Delta: X\n",
    "Gamma: X\n",
    "```\n",
    "Where `X` represents the counts for relevant variants"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "source": [
    "# Open large file into descriptor\n",
    "file_desc = open('data/data_report.jsonl')\n",
    "\n",
    "pang_classifs = []\n",
    "pc_not_found_count = 0\n",
    "\n",
    "# Load one line at a time\n",
    "for line in tqdm(file_desc):\n",
    "    # Append pangolin classification to list\n",
    "    # print(line)\n",
    "    # pang_classifs.append(line['virus']['pangolinClassification'])\n",
    "    line_dict = json.loads(line)\n",
    "    try:\n",
    "        pang_classif = line_dict['virus']['pangolinClassification']\n",
    "        pang_classifs.append(pang_classif)\n",
    "    except KeyError:\n",
    "        pc_not_found_count += 1"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "18ad221785d04bef96415ba79671a7e8"
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "source": [
    "counts = Counter(pang_classifs)\n",
    "print('Pangolin classifications found:', len(pang_classifs))\n",
    "print('Missing pangolin classifications:', pc_not_found_count)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Pangolin classifications found: 429071\n",
      "Missing pangolin classifications: 211\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "source": [
    "# Track the frequency of each variant\n",
    "variant_freqs = {\n",
    "    'Alpha':   0,\n",
    "    'Beta':    0,\n",
    "    'Delta':   0,\n",
    "    'Gamma':   0,\n",
    "}\n",
    "\n",
    "# Map variant classifications to variant names\n",
    "classif_map = {\n",
    "    'B.1.1.7':     'Alpha',\n",
    "    'B.1.351':     'Beta',\n",
    "    'B.1.351.2':   'Beta',\n",
    "    'B.1.351.3':   'Beta',\n",
    "    'B.1.617.2':   'Delta',\n",
    "    'AY.1':        'Delta',\n",
    "    'AY.2':        'Delta',\n",
    "    'AY.3':        'Delta',\n",
    "    'P.1':         'Gamma',\n",
    "    'P.1.1':       'Gamma',\n",
    "    'P.1.2':       'Gamma'\n",
    "}\n",
    "\n",
    "# Manually tally frequencies from counter\n",
    "for classif in classif_map :\n",
    "    variant = classif_map[classif]\n",
    "    classif_freq = counts[classif]\n",
    "    variant_freqs[variant] += classif_freq\n",
    "\n",
    "variant_freqs"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'Alpha': 178423, 'Beta': 584, 'Delta': 4392, 'Gamma': 6632}"
      ]
     },
     "metadata": {},
     "execution_count": 218
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "source": [
    "# Get a list of all variant occurrences using Counter\n",
    "# Display as data frame\n",
    "variant_freq_df = pd.DataFrame(list(variant_freqs.items()))\n",
    "variant_freq_df.columns = ['Variant', 'Frequency']\n",
    "\n",
    "variant_freq_df"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variant</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alpha</td>\n",
       "      <td>178423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beta</td>\n",
       "      <td>584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Delta</td>\n",
       "      <td>4392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gamma</td>\n",
       "      <td>6632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Variant  Frequency\n",
       "0   Alpha     178423\n",
       "1    Beta        584\n",
       "2   Delta       4392\n",
       "3   Gamma       6632"
      ]
     },
     "metadata": {},
     "execution_count": 219
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Find similar viruses.\n",
    "\n",
    "It's often useful to compare viruses to study how similar strains are. While sophisticated algorithms to compare a pair of viruses exist, these are typically computationally intensive and cannot be used to carry out a large number of comparisons. \n",
    "\n",
    "An alternative, albeit less sensitive, approach consists of comparing word counts (called k-mers, where k is the word size) across genomes.  Suppose we have two viruses X and Y, with the following Genomes.\n",
    "```\n",
    "X = \"ACGTAGTGCATGTGTAGCTGTGTAGCTGTAC\"\n",
    "Y = \"ACTAGTGCATGTGTAGCTCTGTAGCTGATAC\"\n",
    "```\n",
    "\n",
    "To compare `X` and `Y`, we first vectorize these genomes by marking the presence of words (k-mers) as a boolean value, 0 if absent and 1 if the word is present. This method assumes that similar genomes will have the same words, which makes sense.\n",
    "\n",
    "This idea, which is referred to as the bag of words model is computationally efficient, making it ideal to vectorize text in big data analytics. Another variant of this model requires replacing the presence and absence by counts for each word.\n",
    "\n",
    "The code below vectorizes an input DNA sequence intro k-mers of size k=2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "source": [
    "# CODE PROVIDED -- DO NOT REMOVE\n",
    "def get_kmer_2(X):\n",
    "    DNA = [\"A\", \"C\", \"G\", \"T\"]\n",
    "    words_size_2 = [\"\".join(dna_prod) for dna_prod in product(DNA, DNA)]\n",
    "    counts = pd.Series([0 for _ in words_size_2], index = words_size_2)\n",
    "    words_in_X = set([X[i:i+2] for i in range(0, len(X)-1)])\n",
    "    counts[list(words_in_X)] = 1\n",
    "    return counts    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "source": [
    "# CODE PROVIDED -- DO NOT REMOVE\n",
    "# X has 3 words of size 2 (AC, CG, GT)\n",
    "X = \"ACGT\"\n",
    "get_kmer_2(\"ACGT\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "AA    0\n",
       "AC    1\n",
       "AG    0\n",
       "AT    0\n",
       "CA    0\n",
       "CC    0\n",
       "CG    1\n",
       "CT    0\n",
       "GA    0\n",
       "GC    0\n",
       "GG    0\n",
       "GT    1\n",
       "TA    0\n",
       "TC    0\n",
       "TG    0\n",
       "TT    0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 221
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The function below takes a dictionary of sequences' counts as a `pandas Series` and prints it using HTML Table, which you might agree is nicer to visualize."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "source": [
    "# CODE PROVIDED -- DO NOT REMOVE\n",
    "def pretty_print_counts(counts_dict):\n",
    "    list_of_count = [data.to_list() for data in counts_dict.values()]\n",
    "    list_of_indices = [x for x in counts_dict.keys()]\n",
    "    list_of_columns = list(counts_dict.values())[0].index.to_list()\n",
    "    df_single_level_cols = pd.DataFrame(list_of_count,\n",
    "                                        index=[x for x in counts_dict.keys()],\n",
    "                                       columns = list_of_columns)    \n",
    "    return df_single_level_cols "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "source": [
    "# CODE PROVIDED -- DO NOT REMOVE\n",
    "X = \"ACGTACGTACGTACGT\"\n",
    "Y = \"ACGTACAAACGTACGT\"\n",
    "Z = \"TTTTACAAACGTTTTT\"\n",
    "\n",
    "counts_dict = {\"X\": get_kmer_2(X), \"Y\": get_kmer_2(Y), \"Z\": get_kmer_2(Z)}\n",
    "pretty_print_counts(counts_dict)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AA</th>\n",
       "      <th>AC</th>\n",
       "      <th>AG</th>\n",
       "      <th>AT</th>\n",
       "      <th>CA</th>\n",
       "      <th>CC</th>\n",
       "      <th>CG</th>\n",
       "      <th>CT</th>\n",
       "      <th>GA</th>\n",
       "      <th>GC</th>\n",
       "      <th>GG</th>\n",
       "      <th>GT</th>\n",
       "      <th>TA</th>\n",
       "      <th>TC</th>\n",
       "      <th>TG</th>\n",
       "      <th>TT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AA  AC  AG  AT  CA  CC  CG  CT  GA  GC  GG  GT  TA  TC  TG  TT\n",
       "X   0   1   0   0   0   0   1   0   0   0   0   1   1   0   0   0\n",
       "Y   1   1   0   0   1   0   1   0   0   0   0   1   1   0   0   0\n",
       "Z   1   1   0   0   1   0   1   0   0   0   0   1   1   0   0   1"
      ]
     },
     "metadata": {},
     "execution_count": 223
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q.6\n",
    "\n",
    "Write a function that computes the Jaccard similarity between two feature vectors, A and B. If you recall, Jaccard similarity is computed as:\n",
    "\n",
    "$$J(A,B) = \\frac{A \\cap B}{A \\cup B}$$\n",
    "\n",
    "In other words, the number of items shared by `A` and `B` over the set of all items in `A` or `B`.\n",
    "\n",
    "For example, for `A= get_kmer_2(X)` and Y = get_kmer_2(B) above,\n",
    "\n",
    "$$\n",
    "J(A,B) = \\frac{4}{6}\n",
    "$$\n",
    "\n",
    "Your function should have the following signature: \n",
    "\n",
    "`jaccard(A, B)`\n",
    "\n",
    "Where `A` and `B` are `pandas Series`\n",
    "\n",
    "\n",
    "Test your function using the code below to make sure it's correct."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "source": [
    "def jaccard(A, B):\n",
    "    and_count = 0\n",
    "    or_count = 0\n",
    "    # Iterate through A and B with an index.  At each index, increment or_count\n",
    "    # if either A or B is 1 and increment and_count if both = 1\n",
    "    for i in range(len(A)):\n",
    "        if A[i] or B[i]:\n",
    "            or_count += 1\n",
    "            if A[i] and B[i]:\n",
    "                and_count += 1\n",
    "    return and_count / or_count"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "source": [
    "# TEST PROVIDED -- DO NOT REMOVE\n",
    "A = get_kmer_2(X)\n",
    "B = get_kmer_2(Y)\n",
    "assert jaccard(A, B) == 4/6"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q.7 \n",
    "\n",
    "* Compute the jaccard similarity for the pairs of sequences `(X, Y)`, `(X, Z)`, `(Y, Z)`. \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "source": [
    "C = get_kmer_2(Z)\n",
    "\n",
    "# Store values in a dict to print later\n",
    "jaccard_dict_2 = {\n",
    "    'Sequence Pair' : ['(X, Y)', '(X, Z)', '(Y, Z)'],\n",
    "    'Jaccard Similarity' : [jaccard(A, B), jaccard(A, C), jaccard(B, C)]\n",
    "}\n",
    "\n",
    "# Print as data frame\n",
    "pd.DataFrame(jaccard_dict_2)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence Pair</th>\n",
       "      <th>Jaccard Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(X, Y)</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(X, Z)</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Y, Z)</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sequence Pair  Jaccard Similarity\n",
       "0        (X, Y)            0.666667\n",
       "1        (X, Z)            0.571429\n",
       "2        (Y, Z)            0.857143"
      ]
     },
     "metadata": {},
     "execution_count": 226
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q.8\n",
    "\n",
    "The vectors representing the presence and absence of words in both `Y` and `Z` are very similar (Jaccard = 0.85), despite major differences at the DNA level between these two sequences. This is because the words are small -- it is as if you were comparing a history book with a book on Python using words of size 2. It's very likely that both books will contain the same words of size 2. Increasing the size of `k` will produce substantial differences. \n",
    "\n",
    "Change the function `get_kmer_2` so that given a sequence `X` and a k-mer size `k`, the function returns a boolean vector of all the words of size `k` in `X`. Cal the function `get_kmer`\n",
    "\n",
    "\n",
    "\n",
    "The following code can be used to generate all DNA words of size `k`\n",
    "```pyton\n",
    "words_size_k = [\"\".join(prod) for prod in product(*([DNA]*k))]\n",
    "```\n",
    "\n",
    "Once done, use the code below to test your function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "source": [
    "# Based on get_kmer_2\n",
    "def get_kmer(X, k):\n",
    "    DNA = [\"A\", \"C\", \"G\", \"T\"]\n",
    "    words_size_k = [\"\".join(dna_prod) for dna_prod in product(DNA, repeat=k)]\n",
    "\n",
    "    counts = pd.Series([0 for _ in words_size_k], index = words_size_k)\n",
    "\n",
    "    # Make a list of words of length k in X\n",
    "    words_in_X = []\n",
    "    for i in range(len(X)-k+1):\n",
    "        words_in_X.append(X[i:i+k])\n",
    "    \n",
    "    counts[list(words_in_X)] = 1\n",
    "    return counts"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "source": [
    "# TEST PROVIDED -- DO NOT REMOVE\n",
    "X = \"ACGTGATGATTG\"\n",
    "\n",
    "counts = get_kmer(X, k=1)\n",
    "assert counts.tolist() == [1,1,1,1]\n",
    "\n",
    "counts = get_kmer(X, k=3)\n",
    "assert (counts[[\"ACG\", \"CGT\", \"GTG\", \"TGA\", \"GAT\", \"ATG\", \"ATT\", \"TTG\"]] == 1).sum()  == 8\n",
    "assert (counts.drop([\"ACG\", \"CGT\", \"GTG\", \"TGA\", \"GAT\", \"ATG\", \"ATT\", \"TTG\"]) == 0).sum()  == 56"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q.9\n",
    "\n",
    "* Compute the Jaccard similarity for the pairs `(X, Y)`, `(X, Z)`, `(Y, Z)` using `k= 5`\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "source": [
    "X = \"ACGTACGTACGTACGT\"\n",
    "Y = \"ACGTACAAACGTACGT\"\n",
    "Z = \"TTTTACAAACGTTTTT\"\n",
    "\n",
    "A = get_kmer(X, 5)\n",
    "B = get_kmer(Y, 5)\n",
    "C = get_kmer(Z, 5)\n",
    "\n",
    "# Store values in a dict to print later\n",
    "jaccard_dict_5 = {\n",
    "    'Sequence Pair' : ['(X, Y)', '(X, Z)', '(Y, Z)'],\n",
    "    'Jaccard Similarity' : [jaccard(A, B), jaccard(A, C), jaccard(B, C)]\n",
    "}\n",
    "\n",
    "# Display as data frame\n",
    "pd.DataFrame(jaccard_dict_5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence Pair</th>\n",
       "      <th>Jaccard Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(X, Y)</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(X, Z)</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Y, Z)</td>\n",
       "      <td>0.294118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sequence Pair  Jaccard Similarity\n",
       "0        (X, Y)            0.400000\n",
       "1        (X, Z)            0.000000\n",
       "2        (Y, Z)            0.294118"
      ]
     },
     "metadata": {},
     "execution_count": 229
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The appropriate word size varies with the length of the text, with larger words depicting similarity more accurately. However, large values of `k` are:\n",
    "1. More computationally intensive to compute. With k = 12, there are $4^{12} = 16,777,216$ words to compute for each sequence.\n",
    "\n",
    "2. More likely to skew the distance between fairly similar sequences. For example `k=8`, the Jaccard index between `X` and `Y` is `0`, even though `X` and `Y` have only two mismatching characters. While this is an extreme case due to the fact that X and Y are short, the logic applies to longer sequences and larger values of `k`\n",
    "\n",
    "\n",
    "![](https://www.dropbox.com/s/rhw5szbiohsqu7w/mismatches.png?dl=1)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The code I used is provided as a reference below. The code took 7 hours to complete on a single machine and approximately 12 minutes on a larger server with 72 cores and 1TB of RAM. To parallelize the execution, I split the file into files that contain 1000 sequences each and used GNU Parallel to run each file on a single CPU core."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "source": [
    "# CODE PROVIDED FOR ILLUTRATION -- DO NOT REMOVE\n",
    "# RUNNING LOCALLY MAY TAKE A LONG TIME TO COMPLETE\n",
    "\n",
    "# k = 8 \n",
    "# DNA = [\"A\", \"C\", \"G\", \"T\"]\n",
    "# words_size_k = [\"\".join(prod) for prod in product(*([DNA]*k))]\n",
    "\n",
    "    \n",
    "# def get_kmer_mod(X):\n",
    "#     counts = pd.Series([0 for _ in words_size_k], index = words_size_k)\n",
    "#     words_in_X = set([X[i:i+k] for i in range(0, len(X)-k+1)])\n",
    "#     counts[list(words_in_X)] = 1\n",
    "#     return counts   \n",
    "\n",
    "# def replace_bad_nucs(seq):\n",
    "#     for character in ['W', 'K', \"Y\", \"M\", 'H']:\n",
    "#         seq = seq.replace(character, 'A') \n",
    "        \n",
    "#     for character in ['R', 'S', 'D', \"V\", \"B\"]:\n",
    "#         seq = seq.replace(character, 'C') \n",
    "        \n",
    "#     seq = seq.replace(\"N\", '') \n",
    "    \n",
    "#     return seq\n",
    "\n",
    "# all_counts = []\n",
    "# all_names = []\n",
    "# with tqdm(total=1000) as pbar:\n",
    "#     for record in SeqIO.parse(\"myseq0.fa\", 'fasta'):\n",
    "#         all_names.append(record.id)\n",
    "#         seq = replace_bad_nucs(str(record.seq))\n",
    "\n",
    "#         counts = get_kmer_mod(seq)\n",
    "#         all_counts.append(counts)\n",
    "#         pbar.update(1)\n",
    "    \n",
    "# kmer_counts = pd.DataFrame(all_counts, index = all_names)\n",
    "# kmer_counts.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hashing Sequences\n",
    "\n",
    "We are interested in finding pairs of sequences that are very similar. However, comparing the sequences pairwise is not tractable since it would require carrying out $429282 * (429282 - 1) / 2 = 92,141,303,121$ comparisons.\n",
    "\n",
    "Instead, we will use the hashing-based approach covered in class. Rather than hashing a sequence over all k-mers, we will only compute the hash for a subset of k-mers. we will repeat the operation n times to avoid that similar sequences are assigned to different bins due to a single, rare mismatch.\n",
    "\n",
    "This, as discussed in class, is computationally more efficient compared to computing all pairwise sequences. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q.10 \n",
    "\n",
    "Write a function that takes a `pandas  Series` and a subset of columns and returns the hash computed on the subset of columns. Call this function`hash_on_subset`.\n",
    "\n",
    "As an example, consider all words with a size of 2 as follows \n",
    "\n",
    "|\t|AA\t|AC\t|AG\t|AT\t|CA | CC| CG| CT| GA| GC| GG| GT| TA| TC| TG| TT|\n",
    "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
    "| A\t|0\t|1\t|0\t|0\t|0\t|0\t|1\t|0\t| 0 |0\t|0\t| 1 |1  |0\t|0\t|0  |\n",
    "\n",
    "```python\n",
    "hash_on_subset(A, [\"AC\", \"CG\", \"CT\", \"GT\", \"TA\"]) \n",
    "```\n",
    "\n",
    "is equivalent to:\n",
    "\n",
    "```python\n",
    "hash((1, 1, 0, 1, 1)) == 5085477689562523216\n",
    "```\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X = \"ACGTACGTACGTACGT\"\n",
    "A = get_kmer_2(X)\n",
    "\n",
    "def hash_on_subset(k_mers, subset):\n",
    "    # For each subset item, check for the binary presence value\n",
    "    ## in the kmers dictionary.\n",
    "    subset_vals = ()\n",
    "    for e in subset:\n",
    "        # Check that the key exists in k_mers\n",
    "        try:\n",
    "            subset_vals = subset_vals + (k_mers[e],)\n",
    "        except KeyError:\n",
    "            print('Value not found for key ', e)\n",
    "    return hash(subset_vals)\n",
    "\n",
    "assert hash_on_subset(A, [\"AC\", \"CG\", \"CT\", \"GT\", \"TA\"]) == 5085477689562523216"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "The method `sample` from the random module `m` words from a list\n",
    "\n",
    "For example, running:\n",
    "```python\n",
    "random.sample( [\"A\", \"C\", \"G\", \"T\"], 2 )\n",
    "```\n",
    "returns\n",
    "```\n",
    "['A', 'C']\n",
    "```\n",
    "The returned subset may be different for you.\n",
    "\n",
    "* The code below randomly selects `m=20` k-mers we will use to compare the genomes\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "source": [
    "random.sample( [\"A\", \"C\", \"G\", \"T\"], 2 )"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['G', 'T']"
      ]
     },
     "metadata": {},
     "execution_count": 232
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "source": [
    "k=8\n",
    "DNA = [\"A\", \"C\", \"G\", \"T\"]\n",
    "words_size_k = [\"\".join(prod) for prod in product(*([DNA]*k))]\n",
    "\n",
    "m=20\n",
    "subset_kmers = random.sample(words_size_k, m)\n",
    "\n",
    "subset_kmers"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['ACGTATTC',\n",
       " 'CATAAAGG',\n",
       " 'GTGAGCTC',\n",
       " 'TTAGGTCA',\n",
       " 'CTGGTCTA',\n",
       " 'TAAGGACT',\n",
       " 'AGTAAAAG',\n",
       " 'TGACCCTG',\n",
       " 'GAATTGTT',\n",
       " 'CGTAGGTT',\n",
       " 'ACGGTCTG',\n",
       " 'TACAGCCA',\n",
       " 'TGGGCACC',\n",
       " 'TGAATGTC',\n",
       " 'TGCAGTTG',\n",
       " 'CGCTCTGC',\n",
       " 'ACTAGAAA',\n",
       " 'GTCTGGGT',\n",
       " 'TGCTGATG',\n",
       " 'GTTGACTT']"
      ]
     },
     "metadata": {},
     "execution_count": 233
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q.12\n",
    "\n",
    "\n",
    "Apply the function `hash_subset` to all the rows of `all_kmers_df`. The data science (*vectorized*) way to do so is using the `apply` method available on a `pandas DataFrame` instead of using for loops. For example, given a DataFrame `df` such that:\n",
    "\n",
    "```\n",
    "df = pd.DataFrame([[1,2,3], [4,5,6]])\n",
    "\n",
    "```\n",
    "then \n",
    "```\n",
    "df.apply(max, args=[] axis=1)\n",
    "```\n",
    "applies the `max()` function on each row (`axis = 1`). Here, `args` is empty since `max` does not take any additional arguments.\n",
    "\n",
    "The example below shows how to use `apply` when the function requires additional arguments. In this example, we apply a function that sums all the values of a row and adds to the sum an offset (2 by default)\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "source": [
    "# EXAMPLE CODE PROVIDED -- DO NOT REMOVE\n",
    "def add_val_to_sum(x, offset=2):\n",
    "    return x.sum() + offset\n",
    "    \n",
    "df = pd.DataFrame([[1,2,3], [4,5,6]])\n",
    "\n",
    "print(\"The sum of rows + an offset of 5 is:\")\n",
    "print(df.apply(add_val_to_sum, args=[5], axis=1))\n",
    "\n",
    "print(\"The sum of rows + an offset of 10 is:\")\n",
    "print(df.apply(add_val_to_sum, args=[10], axis=1))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The sum of rows + an offset of 5 is:\n",
      "0    11\n",
      "1    20\n",
      "dtype: int64\n",
      "The sum of rows + an offset of 10 is:\n",
      "0    16\n",
      "1    25\n",
      "dtype: int64\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "source": [
    "# Loaded locally.  May need to reload for grading\n",
    "all_kmers_df = pd.read_csv('data/all_kmers_10k.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "all_kmers_hashes = all_kmers_df.apply(hash_on_subset, args=[subset_kmers], axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bins = all_kmers_hashes.unique().shape[0]\n",
    "\n",
    "print('There are', all_kmers_hashes.shape[0], 'hashes and', bins, 'bins.')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "There are 9988 hashes and 13 bins.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q.13\n",
    "\n",
    "\n",
    "Use `apply()` to apply `hash_subset` and compute the hash values for all the rows of `all_kmers_df` over `subset_kmers`\n",
    "\n",
    "* Create a dict by parsing the results to group sequences that yield the same hash under the same bins. Each key in the dict should be a key and each value is a list of sequences that have the same value.\n",
    "\n",
    "For example, in the dictionary below, X and Y have the same hash value (123456) over a given subset of kmers, whereas Z has a different hash over the same subsets.\n",
    "\n",
    "```\n",
    "{\"123456\": [X,Y], \"654321\": [Z]}\n",
    "```\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# No need to use apply() again, since entries in all_kmers_hashes \n",
    "## will correspond with their entries (sequences) from all_kmers_df's first row.\n",
    "# Thus, we can build our dictionary by iterating through the indices of each\n",
    "\n",
    "# Get the sequences from all_kmers_df's first row\n",
    "sequences = all_kmers_df.iloc[:,0]\n",
    "\n",
    "# Check: same number of sequences and hashes\n",
    "assert sequences.shape == all_kmers_hashes.shape\n",
    "\n",
    "hashes_sequences_dict = {}\n",
    "\n",
    "for i in range(len(sequences)):\n",
    "    hash = all_kmers_hashes[i]\n",
    "    seq = sequences[i]\n",
    "    # If the hash is already in the dictionary, append the sequence to the dict\n",
    "    try:\n",
    "        hashes_sequences_dict[hash].append(seq)\n",
    "    # Otherwise, this is a new hash, so create key and value for it\n",
    "    except KeyError:\n",
    "        hashes_sequences_dict[hash] = [seq]\n",
    "\n",
    "# Check: now have the same number of bins as above.\n",
    "assert bins == len(hashes_sequences_dict)\n",
    "\n",
    "# Print bin length\n",
    "for hash in hashes_sequences_dict:\n",
    "    sequence_count = len(hashes_sequences_dict[hash])\n",
    "    print('Hash', hash, 'has', sequence_count, 'associated sequences')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Hash -8519770814583746273 has 9863 associated sequences\n",
      "Hash 5135793966126904176 has 25 associated sequences\n",
      "Hash -8295205459950974670 has 11 associated sequences\n",
      "Hash 4188576464044593256 has 12 associated sequences\n",
      "Hash -7340216237926992554 has 56 associated sequences\n",
      "Hash -4201623696827236196 has 3 associated sequences\n",
      "Hash -4595283102120103028 has 6 associated sequences\n",
      "Hash -3626696785611049907 has 4 associated sequences\n",
      "Hash -2821262964395235296 has 1 associated sequences\n",
      "Hash 180475120631573744 has 1 associated sequences\n",
      "Hash 3551864796409958425 has 2 associated sequences\n",
      "Hash -4712598908341122622 has 2 associated sequences\n",
      "Hash 2662595652204096353 has 2 associated sequences\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q. 15\n",
    "\n",
    "Here we use the presence and absence of words, i.e., a vector of booleans, to encode a sequence. The problem with this approach is that it considers the sequences to be identical, even if their word counts differ substantially. For example, given the sequence `X`, `Y` and `Z` as follows\n",
    "```\n",
    "X = ATAGATAGATAGATAGATT\n",
    "Y = ATAGATAGATAGATAGATT\n",
    "Z = ATAGATTTTTTTTTTTTTT\n",
    "```\n",
    "With k =2, all three sequences mach on their vector of word presence/absense."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X = \"ATAGATAGATAGATAGATT\"\n",
    "Y = \"ATAGATAGATAGATAGATT\"\n",
    "Z = \"ATAGATTTTTTTTTTTTTT\"\n",
    "\n",
    "counts_dict = {\"X\": get_kmer(X, k=2), \"Y\": get_kmer(Y, k=2), \"Z\": get_kmer(Z, k=2)}\n",
    "pretty_print_counts(counts_dict)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AA</th>\n",
       "      <th>AC</th>\n",
       "      <th>AG</th>\n",
       "      <th>AT</th>\n",
       "      <th>CA</th>\n",
       "      <th>CC</th>\n",
       "      <th>CG</th>\n",
       "      <th>CT</th>\n",
       "      <th>GA</th>\n",
       "      <th>GC</th>\n",
       "      <th>GG</th>\n",
       "      <th>GT</th>\n",
       "      <th>TA</th>\n",
       "      <th>TC</th>\n",
       "      <th>TG</th>\n",
       "      <th>TT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AA  AC  AG  AT  CA  CC  CG  CT  GA  GC  GG  GT  TA  TC  TG  TT\n",
       "X   0   0   1   1   0   0   0   0   1   0   0   0   1   0   0   1\n",
       "Y   0   0   1   1   0   0   0   0   1   0   0   0   1   0   0   1\n",
       "Z   0   0   1   1   0   0   0   0   1   0   0   0   1   0   0   1"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Comparing these sequences based on word counts, X is much more similar to Y than it is to Z"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_kmer_counts(X, k):\n",
    "    DNA = [\"A\", \"C\", \"G\", \"T\"]\n",
    "    words_size_k = [\"\".join(prod) for prod in product(*([DNA]*k))]\n",
    "    counts = pd.Series([0 for _ in words_size_k], index = words_size_k)\n",
    "    counts_words_in_x = Counter([X[i:i+k] for i in range(0, len(X)-k+1)])\n",
    "    counts.update(counts_words_in_x)\n",
    "    return counts    \n",
    "\n",
    "\n",
    "counts_dict = {\"X\": get_kmer_counts(X, k=2), \"Y\": get_kmer_counts(Y, k=2), \"Z\": get_kmer_counts(Z, k=2)}\n",
    "pretty_print_counts(counts_dict)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AA</th>\n",
       "      <th>AC</th>\n",
       "      <th>AG</th>\n",
       "      <th>AT</th>\n",
       "      <th>CA</th>\n",
       "      <th>CC</th>\n",
       "      <th>CG</th>\n",
       "      <th>CT</th>\n",
       "      <th>GA</th>\n",
       "      <th>GC</th>\n",
       "      <th>GG</th>\n",
       "      <th>GT</th>\n",
       "      <th>TA</th>\n",
       "      <th>TC</th>\n",
       "      <th>TG</th>\n",
       "      <th>TT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AA  AC  AG  AT  CA  CC  CG  CT  GA  GC  GG  GT  TA  TC  TG  TT\n",
       "X   0   0   4   5   0   0   0   0   4   0   0   0   4   0   0   1\n",
       "Y   0   0   4   5   0   0   0   0   4   0   0   0   4   0   0   1\n",
       "Z   0   0   1   2   0   0   0   0   1   0   0   0   1   0   0  13"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Given an example (vectors) to justify why hashing is not ideal with counts.\n",
    "  * Use any means you think are useful to illustrate your point (e.g.: figure, simulation (yes, please!))\n",
    " \n",
    "* Describe how the random project approach discussed in class can help solve the issue discussed\n",
    "  * Use code to illustrate how random projection works in the following example.\n",
    "    * I.e., provide code to provide an example where `X` and `Y` are assigned to the same bin and `Y` is assigned to a different bin.\n",
    "    * You can choose any vector values as needed \n",
    " \n",
    "```python\n",
    "X = [1,2]\n",
    "Y = [2,2]\n",
    "Z = [5,1]\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hashing won't work well with counts since the hash function will produce too many similar values, and bins will be heavily filled.  This will lead to search times approaching O(n), whereas the benefit of hashing is to keep search times close to O(1)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "With random projections, choosing a random line means that the projection of two normally similarly-hashing points will be a lot less likely to have the same hash, thus keeping bin fill low and search times closer to O(1) than with standard hashing."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit (conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "e134e05457d34029b6460cd73bbf1ed73f339b5b6d98c95be70b69eba114fe95"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}